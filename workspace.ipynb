{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from typing import Tuple, List, Optional\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Input,BatchNormalization,Layer\n",
    "# from tensorflow.keras import regularizers\n",
    "# import  tensorflow.keras.backend  as K\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# import math\n",
    "# import plotly.express as px\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self,image_paths: List[str],classes: List[str],root_dir:str,transforms:Optional[transforms.Compose]=None):\n",
    "\n",
    "        self.image_paths = [os.path.join(root_dir, image_path)+\".jpg\" for image_path in image_paths]\n",
    "        self.labels = [image_path.split(\"/\")[0] for image_path in image_paths]\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.class_to_idx[label]\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "root_dir = \"data/food-101/images\"\n",
    "number_of_classes = 3\n",
    "batch_size = 2\n",
    "\n",
    "classes = [i.strip() for i in open(\"data/food-101/meta/classes.txt\",\"r\").readlines()]\n",
    "print(classes[:2])\n",
    "\n",
    "train_image_paths = [i.strip() for i in open(\"data/food-101/meta/train.txt\",\"r\").readlines()]\n",
    "print(train_image_paths[:2])\n",
    "\n",
    "test_image_paths = [i.strip() for i in open(\"data/food-101/meta/test.txt\",\"r\").readlines()]\n",
    "\n",
    "print(f\"train image count: {len(train_image_paths)} test image count: {len(test_image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = [i.split(\"/\")[0] for i in train_image_paths]\n",
    "\n",
    "chosen_class = list(set(cl))[:number_of_classes]\n",
    "classes = chosen_class\n",
    "train_image_paths = [i for i in train_image_paths if i.split(\"/\")[0] in chosen_class]\n",
    "test_image_paths = [i for i in test_image_paths if i.split(\"/\")[0] in chosen_class]\n",
    "\n",
    "len(train_image_paths), len(test_image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(image_paths=train_image_paths,classes=classes,root_dir=root_dir,transforms=transforms)\n",
    "test_dataset = FoodDataset(image_paths=test_image_paths,classes=classes,root_dir=root_dir,transforms=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_dataset[0][0].permute(1, 2, 0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256,512),#(128 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layer\n",
    "     \n",
    "        \n",
    "        # Apply fully connected layers with ReLU and dropout\n",
    "        x = self.fc(x) \n",
    "        return x\n",
    "\n",
    "model = VisionModel(num_classes=number_of_classes)\n",
    "model.to(device)\n",
    "#out = model(torch.randn(1,3,224,224))\n",
    "#out.shape\n",
    "\n",
    "summary(\n",
    "    model,input_size=(3,224,224),batch_size=1\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss =0\n",
    "train_acc = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train step\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for step, (X, y) in  tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        #break\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()       # 2\n",
    "        loss.backward()             # 3\n",
    "        optimizer.step()            # 4\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    \n",
    "    # val step\n",
    "    model.eval()\n",
    "    val_loss = 0 \n",
    "    val_acc = 0\n",
    "    for step, (X, y) in  tqdm(enumerate(test_dataloader),total=len(test_dataloader)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataset)\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_acc /= len(test_dataset)\n",
    "\n",
    "    print(f\"epoch: {epoch}, train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f},val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}\")\n",
    "    \n",
    "    \n",
    "    #    break\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n",
    "img_size = (224,224)\n",
    "class_size = len(os.listdir(\"data/min_celeb_face\"))\n",
    "class_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Example: (uncomment and replace with actual data loading)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train[..., tf.newaxis].astype('float32') / 255.0\n",
    "x_test = x_test[..., tf.newaxis].astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "# # Load datasets\n",
    "# train_ds = keras.utils.image_dataset_from_directory(\n",
    "#     \"data/min_celeb_face\",\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"training\",\n",
    "#     seed=123,\n",
    "#     image_size=img_size,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# val_ds = keras.utils.image_dataset_from_directory(\n",
    "#     \"data/min_celeb_face\",\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"validation\",\n",
    "#     seed=123,\n",
    "#     image_size=img_size,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# # Preprocess datasets\n",
    "# def preprocess_dataset(dataset):\n",
    "#     def preprocess(images, labels):\n",
    "#         labels = tf.one_hot(labels, depth=class_size)\n",
    "#         return ((images,labels), labels)\n",
    "#     return dataset.map(preprocess)\n",
    "\n",
    "# train_ds = preprocess_dataset(train_ds)\n",
    "# val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# # Example to iterate through the dataset and check shapes\n",
    "# for inputs, labels in train_ds.take(1):\n",
    "#     print(inputs[0].shape)\n",
    "#     print(inputs[1].shape)\n",
    "#     print(labels.shape)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(Layer):\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.5, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[0][-1], self.n_classes),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        # Normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # Normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # Dot product\n",
    "        logits = tf.matmul(x, W)\n",
    "        # Add margin\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        \n",
    "        # Replace logits for the target classes with the margin-adjusted logits\n",
    "        logits = y * target_logits + (1 - y) * logits\n",
    "\n",
    "        # Feature re-scale\n",
    "        logits *= self.s\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.n_classes)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ArcFace, self).get_config()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'regularizer': self.regularizer\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class CosFace(Layer):\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.35, regularizer=None, **kwargs):\n",
    "        super(CosFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CosFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[0][-1], self.n_classes),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        # Normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # Normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # Dot product\n",
    "        logits = tf.matmul(x, W)\n",
    "        # Add margin\n",
    "        target_logits = logits * (1 - y) + (self.th * y + self.mm) * y\n",
    "        # Feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a simple model\n",
    "def simple_model(input_shape = (224,224,3)):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    label_layer = Input(shape=(class_size,))\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = ArcFace(n_classes=class_size)([x, label_layer])\n",
    "\n",
    "    model = Model([input_layer, label_layer], output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the VGG-like model\n",
    "def vgg_block(x, num_filters, num_convs):\n",
    "    for _ in range(num_convs):\n",
    "        x = Conv2D(num_filters, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def vgg8_arcface(args,input_shape=(224,224,3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    y = Input(shape=(class_size,))\n",
    "\n",
    "    x = vgg_block(inputs, 16, 2)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = vgg_block(x, 64, 2)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = vgg_block(x, 128, 2)\n",
    "    x = vgg_block(x, 256, 2)\n",
    "    \n",
    "    x =  keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    #x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(args.num_features, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(args.weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = ArcFace(n_classes=class_size, regularizer=regularizers.l2(args.weight_decay))([x, y])\n",
    "\n",
    "    return Model([inputs, y], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arguments\n",
    "class Args:\n",
    "    num_features = 3\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "# Create the model\n",
    "model = vgg8_arcface(args)\n",
    "#model = simple_model()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming x_train, y_train, x_test, and y_test are your datasets\n",
    "# Ensure y_train and y_test are one-hot encoded\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a checkpoint callback\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[checkpoint])\n",
    "\n",
    "# model.fit([x_train, y_train],\n",
    "#           y_train,\n",
    "#           batch_size=32,\n",
    "#           epochs=10,\n",
    "#           verbose=1,\n",
    "#           validation_data=([x_test, y_test], y_test),\n",
    "#           callbacks=[tf.keras.callbacks.ModelCheckpoint('model.hdf5', verbose=1, save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.get_layer('dense').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "image_features = []\n",
    "labels = []\n",
    "\n",
    "for i,t in val_ds.take(len(val_ds)):\n",
    "\n",
    "    features = feature_extractor([i[0], i[1]])\n",
    "    image_features.append(features.numpy())\n",
    "    labels.extend(np.argmax(t, axis=1))\n",
    "\n",
    "image_features = np.concatenate(image_features, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=2500)\n",
    "tsne_results = tsne.fit_transform(image_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract Features and Perform t-SNE\n",
    "\n",
    "\n",
    "# # Extract features\n",
    "# image_features = []\n",
    "# labels = []a\n",
    "\n",
    "# for i in range(len(x_test[:5000])):\n",
    "#     features = feature_extractor([x_test[i:i+1], y_test[i:i+1]])\n",
    "#     image_features.append(features.numpy())\n",
    "#     labels.append(np.argmax(y_test[i]))\n",
    "\n",
    "# image_features = np.concatenate(image_features, axis=0)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# # Perform t-SNE\n",
    "# tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=250)\n",
    "# tsne_results = tsne.fit_transform(image_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title('t-SNE of image featuresq')\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.show()# Plot t-SNE results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(x=tsne_results[:, 0], y=tsne_results[:, 1], z=tsne_results[:, 2], color=labels, title='t-SNE of image features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=tsne_results[:, 0], y=tsne_results[:, 1], color=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
