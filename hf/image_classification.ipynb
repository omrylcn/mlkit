{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import (ToTensor,Lambda ,Resize,RandomResizedCrop,CenterCrop ,Compose, Normalize,RandomHorizontalFlip,RandomVerticalFlip,RandomRotation)\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    get_scheduler\n",
    ")\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "label_column_name = 'label' # \"label\" or \"labels\"\n",
    "image_column_name = 'image'\n",
    "model_name_or_path = \"google/vit-base-patch16-224\"\n",
    "model_name_or_path = \"apple/mobilevit-small\"\n",
    "trust_remote_code = True\n",
    "ignore_mismatched_sizes = True\n",
    "weight_decay = 0.0\n",
    "learning_rate = 0.001\n",
    "lr_scheduler_type = \"linear\"  # \"linear\" or \"cosine_with_restarts\" \n",
    "max_train_steps = 20000\n",
    "num_warmup_steps = 0\n",
    "overrode_max_train_steps = False\n",
    "per_device_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/home/tuvis/Work/ml_works/mlkit/data/food-101/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= dataset[\"train\"].train_test_split(0.2)\n",
    "label_column_name = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"beans\")\n",
    "\n",
    "labels = dataset[\"train\"].features[label_column_name].names\n",
    "label2id = {label: str(i) for i, label in enumerate(labels)}\n",
    "id2label = {str(i): label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        i2label=id2label,\n",
    "        label2id=label2id,\n",
    "        finetuning_task=\"image-classification\",\n",
    "        trust_remote_code=trust_remote_code,\n",
    "    )\n",
    "\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-small and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 640]) in the checkpoint and torch.Size([101, 640]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "    )\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
    "    trust_remote_code=trust_remote_code,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "size = (256,256)\n",
    "\n",
    "normalize = (\n",
    "        Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\")\n",
    "        else Lambda(lambda x: x)\n",
    "    )\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply _train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[image_column_name]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply _val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) for image in example_batch[image_column_name]\n",
    "    ]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "val_dataset = dataset[\"test\"].with_transform(preprocess_val)\n",
    "\n",
    "\n",
    "\n",
    "    # DataLoaders creation:\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[label_column_name] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=per_device_batch_size,collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset,shuffle=True,batch_size=per_device_batch_size,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2525"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = per_device_batch_size# * accelerator.num_processes * args.gradient_accumulation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1540/20000 [11:26<2:00:49,  2.55it/s]/home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      " 13%|█▎        | 2525/20000 [18:56<2:23:10,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.1594863861386138 | acc : 0.468 | val_acc : 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2912/20000 [25:27<1:43:03,  2.76it/s]  /home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      " 25%|██▌       | 5050/20000 [43:35<1:42:56,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.4483147045173268 | acc : 0.629 | val_acc : 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5697/20000 [53:43<1:44:01,  2.29it/s]  /home/tuvis/miniconda3/envs/hf/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      " 38%|███▊      | 7575/20000 [1:09:14<1:21:57,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 1.2382741916769802 | acc : 0.680 | val_acc : 0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8378/20000 [1:20:08<1:18:01,  2.48it/s]  "
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "progress_bar.update(completed_steps)\n",
    "\n",
    "for i in range(starting_epoch,num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(model):\n",
    "            X = batch[\"pixel_values\"]\n",
    "            y = batch[\"labels\"]\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=X,labels=y)\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            metric.add_batch(predictions=preds, references=y)\n",
    "\n",
    "        #  print(outputs.loss)\n",
    "            total_loss += loss.detach().float()\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            \n",
    "\n",
    "        \n",
    "    tr_acc = metric.compute()['accuracy']\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            X = batch[\"pixel_values\"]\n",
    "            y = batch[\"labels\"]\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=X,labels=y)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            metric.add_batch(predictions=preds, references=y)\n",
    "\n",
    "\n",
    "    val_acc = metric.compute()['accuracy']\n",
    "\n",
    "\n",
    "    print(f\"Epoch {i} loss: {total_loss.item() / len(train_dataloader)} | acc : {tr_acc:.3f} | val_acc : {val_acc:.3f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = batch[\"pixel_values\"]\n",
    "        y = batch[\"labels\"]\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(pixel_values=X,labels=y)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "       # metric.add_batch(predictions=preds, references=y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pred :   \",preds)\n",
    "print(\"target : \",y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_dataset.shuffle():\n",
    "    \n",
    "    \n",
    "    input_data= val_transforms(i[\"image\"])\n",
    "    input_label = i[\"labels\"]\n",
    "    input_data= torch.unsqueeze(input_data,dim=0)\n",
    "    input_data.shape\n",
    "\n",
    "\n",
    "    output = model(input_data.to(device))\n",
    "    pred = output.logits.argmax(-1).item()\n",
    "    print(f\" name: {labels[input_label]} real class: {input_label}, predicted class: {pred} \")\n",
    "\n",
    "    plt.imshow(i[\"image\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "metric(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "train[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDataset(Dataset):\n",
    "    def __init__(self,hf_dataset,feature_name=\"image\",target_name=\"labels\") -> None:\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.feature_name = feature_name\n",
    "        self.target_name = target_name \n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.hf_dataset[self.feature_name][index]\n",
    "        target = self.hf_dataset[self.target_name][index]\n",
    "\n",
    "        image = self.process(image)\n",
    "\n",
    "        return image,target\n",
    "    def process (self,image):\n",
    "        image = ToTensor()(image)\n",
    "        return image\n",
    "        \n",
    "\n",
    "train_dataset = HFDataset(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        im = Image.open(f)\n",
    "        return im.convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ViT model and image processor\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][\"image\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
